{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "debb9fc2-0db9-4bd4-949a-c9083da8b2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Question- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e306f6c4-9d8f-4f32-930d-aaca1fd11392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n|  src|  dest|tot_time|\n+-----+------+--------+\n|delhi|   goa|      14|\n|  goa|mumbai|      10|\n|delhi|mumbai|      12|\n+-----+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# date conversion, string conversion to int , total sales by month \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "data= [\n",
    "\n",
    "    (1,\"delhi\",\"goa\",7),\n",
    "    (2,\"mumbai\",\"goa\",5),\n",
    "    (3,\"delhi\",\"mumbai\",6),\n",
    "    (4,\"mumbai\",\"delhi\",6),\n",
    "    (5,\"goa\",\"delhi\",7),\n",
    "    (6,\"goa\",\"mumbai\",5),\n",
    "]\n",
    "schema = StructType ([\n",
    "    StructField (\"route\",IntegerType(),True),\n",
    "    StructField (\"source\",StringType(),True),\n",
    "    StructField (\"destination\",StringType(),True),\n",
    "    StructField (\"time\",IntegerType(),True)\n",
    "    \n",
    "])\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "dt = df.select (\n",
    "        when(col(\"source\") <= col(\"destination\"), col(\"source\")).otherwise(col(\"destination\")).alias(\"src\"),\n",
    "    when(col(\"source\") <= col(\"destination\"), col(\"destination\")).otherwise(col(\"source\")).alias(\"dest\"),\n",
    "    col(\"time\")\n",
    "\n",
    ")\n",
    "\n",
    "dt.groupBy(col(\"src\"),col(\"dest\")).agg(\n",
    "    sum(col(\"time\")).alias(\"tot_time\")\n",
    "\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1feb2f76-f49e-484b-9131-2f54719e4dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5126d47e-53d8-40ab-8e1d-b08ea263b8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------------+-------------------+------------------+\n|id |cust|status|date               |next_date          |time_spent_minutes|\n+---+----+------+-------------------+-------------------+------------------+\n|1  |A   |login |2025-06-25 15:30:00|2025-06-25 16:30:00|60.0              |\n|2  |B   |login |2025-06-25 15:30:00|2025-06-25 17:30:00|120.0             |\n|3  |C   |login |2025-06-25 15:30:00|2025-06-25 18:30:00|180.0             |\n+---+----+------+-------------------+-------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Define your data using datetime objects\n",
    "data = [\n",
    "    (1, \"A\", \"login\",  datetime(2025, 6, 25, 15, 30)),\n",
    "    (2, \"B\", \"login\",  datetime(2025, 6, 25, 15, 30)),\n",
    "    (3, \"C\", \"login\",  datetime(2025, 6, 25, 15, 30)),\n",
    "    (4, \"A\", \"logout\", datetime(2025, 6, 25, 16, 30)),\n",
    "    (5, \"B\", \"logout\", datetime(2025, 6, 25, 17, 30)),\n",
    "    (6, \"C\", \"logout\", datetime(2025, 6, 25, 18, 30))\n",
    "]\n",
    "\n",
    "# 2. Define schema with TimestampType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"cust\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# 3. Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "wind = Window.partitionBy(\"cust\").orderBy(\"date\")\n",
    "\n",
    "# Add next date column using lead\n",
    "df = df.withColumn(\"next_date\", lead(\"date\").over(wind))\n",
    "filt = df.filter(col(\"next_date\").isNotNull())\n",
    "df_final = filt.withColumn(\n",
    "    \"time_spent_minutes\",\n",
    "    (unix_timestamp(\"next_date\") - unix_timestamp(\"date\")) / 60\n",
    ")\n",
    "\n",
    "df_final.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pr_activity_time",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}