{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624aebac-ab7c-4de6-b8f0-96c7f75d3b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### autoloader json to delta table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2842b90-f73d-47b8-ab25-67c5b147d3be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Step 1: Define the schema (recommended for Auto Loader)\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"city\", StringType()) \\\n",
    "    .add(\"country\", StringType()) \\\n",
    "    .add(\"sales\", IntegerType())\n",
    "\n",
    "# Step 2: Read from folder using Auto Loader (cloudFiles)\n",
    "df = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")  # Change to csv, parquet, etc. if needed\n",
    "      .schema(schema)\n",
    "      .load(\"/mnt/incoming-data/\")\n",
    "     )\n",
    "\n",
    "# Step 3: Write to Delta table\n",
    "(df.writeStream\n",
    "   .format(\"delta\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"checkpointLocation\", \"/mnt/incoming-data/_checkpoint\")  # Required\n",
    "   .start(\"/mnt/bronze/sales_city\")  # Path where Delta table data is stored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a26d9f4-8ea0-495e-bb6b-3065b48eb9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# after stream starts create delta table because delta table needs to have data so when stream will start it will write data to checkpoints directory or create earlier delta table having data to upload incoming stream \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales_city\n",
    "USING DELTA\n",
    "LOCATION '/mnt/bronze/sales_city'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "autoloader",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}