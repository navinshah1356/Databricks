# E-Commerce Orders Optimization Pipeline (Databricks + PySpark)

## ğŸ“Œ Overview
This project demonstrates a scalable and optimized data pipeline for e-commerce order data using Databricks and PySpark. The pipeline consists of ingestion, transformation, optimization (Delta Lake), and final analytics-ready data storage.

## ğŸ§° Technologies
- Databricks
- PySpark
- Delta Lake
- Parquet
- Z-Ordering
- GitHub

## ğŸ“‚ Pipeline Stages
1. **Ingestion** â€“ Load raw CSV data
2. **Transformation** â€“ Clean, enrich, and prepare data
3. **Optimization** â€“ Use Delta Lake, partitioning & Z-ordering
4. **Analytics** â€“ Analyze optimized Gold layer

## ğŸ“¦ How to Use
1. Upload all files to Databricks Workspace or run locally with PySpark
2. Run the notebooks in sequence:
   - 01_data_ingestion.py
   - 02_data_cleaning_transformation.py
   - 03_data_optimization.py
   - 04_data_analysis_gold_layer.py

## ğŸ“ Input Data
Included: `data/sample_raw_data.csv`

## âœ… Output
Optimized Delta tables partitioned by `category` and Z-Ordered by `order_date`

## ğŸ‘¨â€ğŸ’» Author
Navin Shah | Data & BI Consultant
