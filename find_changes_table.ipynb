{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed529723-c669-462a-bc92-0a5c7b906d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted Rows:\n+---+------+---+------+-------+\n| id|  name|age|salary|version|\n+---+------+---+------+-------+\n|  8|vishnu| 55| 30000|      1|\n|  5| radha| 20|140000|      1|\n|  3| shyam| 25|130000|      1|\n|  4|  sita| 30|135000|      1|\n|  1|  hari| 45|120000|      1|\n|  2|   ram| 35|125000|      1|\n+---+------+---+------+-------+\n\n‚ùå Deleted Rows:\n+---+-----+---+------+-------+\n| id| name|age|salary|version|\n+---+-----+---+------+-------+\n|  5|radha| 20|140000|      0|\n|  3|shyam| 25|130000|      0|\n|  4| sita| 30|135000|      0|\n|  1| hari| 45|120000|      0|\n|  2|  ram| 35|125000|      0|\n+---+-----+---+------+-------+\n\nüîÅ Updated Rows:\n+---+----+---+------+-------+\n| id|name|age|salary|version|\n+---+----+---+------+-------+\n+---+----+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit ,col\n",
    "\n",
    "# Load the Delta table\n",
    "delta_table = DeltaTable.forName(spark, \"tbl_sal\")\n",
    "\n",
    "# Get the history as a DataFrame\n",
    "history_df = delta_table.history()  # This is a DataFrame\n",
    "\n",
    "\n",
    "# Step 3: Extract latest and previous versions\n",
    "versions = history_df.select(\"version\").orderBy(\"version\", ascending=False).limit(2).collect()\n",
    "latest_version = versions[0][\"version\"]\n",
    "previous_version = versions[1][\"version\"]\n",
    "\n",
    "# Step 4: Read both versions of the Delta table\n",
    "current_df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version).table(\"tbl_sal\").withColumn(\"version\", lit(latest_version))\n",
    "prev_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).table(\"tbl_sal\").withColumn(\"version\", lit(previous_version))\n",
    "\n",
    "# Step 5: Find Inserted rows\n",
    "inserted_rows = current_df.exceptAll(prev_df)\n",
    "print(\"‚úÖ Inserted Rows:\")\n",
    "inserted_rows.show()\n",
    "\n",
    "# Step 6: Find Deleted rows\n",
    "deleted_rows = prev_df.exceptAll(current_df)\n",
    "print(\"‚ùå Deleted Rows:\")\n",
    "deleted_rows.show()\n",
    "\n",
    "join_cond = current_df[\"id\"] == prev_df[\"id\"]\n",
    "updated_rows = current_df.alias(\"curr\").join(\n",
    "    prev_df.alias(\"prev\"),\n",
    "    on=join_cond,\n",
    "    how=\"inner\"\n",
    ").where(\n",
    "    (col(\"curr.name\") != col(\"prev.name\")) |\n",
    "    (col(\"curr.age\") != col(\"prev.age\")) |\n",
    "    (col(\"curr.salary\") != col(\"prev.salary\"))\n",
    ").select(\"curr.*\")\n",
    "\n",
    "print(\"üîÅ Updated Rows:\")\n",
    "updated_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b166c748-b69c-4e71-88ab-3d7ac64b4322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "versioning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}