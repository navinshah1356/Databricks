{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrame Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/sales_data_sample-1.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "\n",
    "df= df.select(col(\"country\"),col(\"year_id\"),col(\"sales\"))\n",
    "#df = df.withColumn(\"date\", date_format(to_date(col(\"year_id\"), \"yyyy-MM-dd HH:mm\"), \"MM-yyyy\"))\n",
    "\n",
    "# Group by 'country' and 'date' and aggregate sales\n",
    "result_df = df.groupBy(\"country\", \"year_id\").agg(\n",
    "    sum(\"sales\").alias(\"tot_sales\")\n",
    ")\n",
    "#.filter(col(\"country\")==\"USA\")\n",
    "result_df.write.format(\"DELTA\").saveAsTable(\"tblcountrysales\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf2dd10-4232-46ff-878f-b1cf60fc8877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[country: string, year_id: string, tot_sales: double]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.table(\"tblcountrysales\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46310aa8-3bc7-4529-88ab-eeda9eb2ec84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added Rows:\n+-------+-------+---------+\n|country|year_id|tot_sales|\n+-------+-------+---------+\n|  nepal|   2025|  1000.12|\n+-------+-------+---------+\n\n‚ùå Deleted Rows:\n+-------+-------+---------+\n|country|year_id|tot_sales|\n+-------+-------+---------+\n+-------+-------+---------+\n\nüîÑ Updated Rows:\n+-------+-------+-------------+-------------+\n|country|year_id|old_tot_sales|new_tot_sales|\n+-------+-------+-------------+-------------+\n+-------+-------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "history_df = spark.sql(\"DESCRIBE HISTORY tblcountrysales\")\n",
    "latest_version = history_df.select(\"version\").orderBy(\"version\", ascending=False).first()[\"version\"]\n",
    "previous_version = latest_version - 1\n",
    "\n",
    "current_df = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version).table(\"tblcountrysales\")\n",
    "previous_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).table(\"tblcountrysales\")\n",
    "\n",
    "\n",
    "# Step 5: Compare changes (records in current not in previous)\n",
    "added_rows = current_df.exceptAll(previous_df)\n",
    "deleted_rows = previous_df.exceptAll(current_df)\n",
    "\n",
    "join_keys = [\"country\", \"year_id\"]\n",
    "\n",
    "joined_df = previous_df.alias(\"old\").join(\n",
    "    current_df.alias(\"new\"),\n",
    "    on=join_keys,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Identify updated rows (rows where any column other than keys changed)\n",
    "updated_rows = joined_df.filter(\n",
    "    col(\"old.tot_sales\") != col(\"new.tot_sales\")  # Adjust for additional columns\n",
    ").select(\n",
    "    *[col(f\"old.{k}\").alias(k) for k in join_keys],\n",
    "    col(\"old.tot_sales\").alias(\"old_tot_sales\"),\n",
    "    col(\"new.tot_sales\").alias(\"new_tot_sales\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Added Rows:\")\n",
    "added_rows.show()\n",
    "\n",
    "print(\"‚ùå Deleted Rows:\")\n",
    "deleted_rows.show()\n",
    "\n",
    "print(\"üîÑ Updated Rows:\")\n",
    "updated_rows.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14c2b54-14e7-42fb-b0bd-7d3244dfbedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "versioning_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}